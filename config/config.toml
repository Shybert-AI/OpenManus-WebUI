# Global LLM configuration
#[llm]
#model = "deepseek-chat"
#base_url = "https://api.deepseek.com/v1"
#api_key = "sk-f3ea7084bef6431eaef505336d1caf0b"
#max_tokens = 1024
#temperature = 0.6
#
## Optional configuration for specific LLM models
#[llm.vision]
#model = "deepseek-chat"
#base_url = "https://api.deepseek.com/v1"
#api_key = "sk-f3ea7084bef6431eaef505336d1caf0b"

#
# Global LLM configuration
[llm]
model = "deepseek-ai/DeepSeek-R1-Distill-Qwen-7B"
base_url = "https://api.siliconflow.cn/v1/"
api_key = "sk-wnfrxjealelxsdnvywkcphqjdaxrqswfkdpwpkohvkgjenzl"
max_tokens = 4096
temperature = 0.6

# Optional configuration for specific LLM models
[llm.vision]
model = "deepseek-ai/DeepSeek-R1-Distill-Qwen-7B"
base_url = "https://api.siliconflow.cn/v1/"
api_key = "sk-wnfrxjealelxsdnvywkcphqjdaxrqswfkdpwpkohvkgjenzl"
